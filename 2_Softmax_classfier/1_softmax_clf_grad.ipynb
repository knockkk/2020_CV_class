{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1. 推导softmax clf 的 analytic gradient，并采用 numpy 实现其计算\n",
    "\n",
    "##### 2. 用numerical gradient 来检验实现的正确性"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([0.2,0.3,0.1,0.5]).reshape(1,4) #x n,3072\n",
    "y = np.array([1]) #n ,1\n",
    "W = np.random.randn(3,4) #10，3072 10 n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_analytic_grad(W,x, y):\n",
    "    scores = x.dot(W.T)\n",
    "    exp_scores = np.exp(scores).reshape(W.shape[0],)\n",
    "    pro_scores = exp_scores / np.sum(exp_scores)\n",
    "    grad = np.zeros(W.shape)\n",
    "    for i in range(W.shape[0]):\n",
    "        if(i==y):\n",
    "            grad[i] = (pro_scores[i] - 1) * x \n",
    "        else:\n",
    "            grad[i] = pro_scores[i] * x\n",
    "    return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(W,x,y):\n",
    "    scores = x.dot(W.T)\n",
    "    exp_scores = np.exp(scores).reshape(W.shape[0],)\n",
    "    pro_scores = exp_scores / np.sum(exp_scores)\n",
    "    return -np.log(pro_scores[y])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_numerical_grad(W,x,y):\n",
    "    h = 1e-5\n",
    "    grad = np.zeros(W.shape)\n",
    "    for i in range(W.shape[0]):\n",
    "        for j in range(W.shape[1]):\n",
    "            fx = loss(W,x,y)\n",
    "            W[i][j] =  W[i][j] + h\n",
    "            fx_h = loss(W,x,y)\n",
    "            grad[i][j] = (fx_h - fx) / h\n",
    "    return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numerical grad: [[ 0.07695148  0.1154274   0.03847585  0.1923796 ]\n",
      " [-0.17587639 -0.26381451 -0.08793815 -0.43969058]\n",
      " [ 0.09892438  0.14838675  0.0494623   0.24731188]]\n",
      "analytic_grad: [[ 0.07695144  0.11542715  0.03847572  0.19237859]\n",
      " [-0.17587631 -0.26381447 -0.08793816 -0.43969078]\n",
      " [ 0.09892488  0.14838731  0.04946244  0.24731219]]\n",
      "diff 3.2698969241601317e-06\n"
     ]
    }
   ],
   "source": [
    "numerical_grad = eval_numerical_grad(W,x,y)\n",
    "analytic_grad = eval_analytic_grad(W,x,y)\n",
    "print('numerical grad:',numerical_grad)\n",
    "print('analytic_grad:',analytic_grad)\n",
    "print('diff',np.sum(np.abs(numerical_grad-analytic_grad)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 loss:2.115123\n",
      "20 loss:0.281491\n",
      "40 loss:0.116665\n",
      "60 loss:0.071513\n",
      "80 loss:0.051179\n",
      "100 loss:0.039732\n",
      "120 loss:0.032423\n",
      "140 loss:0.027363\n",
      "160 loss:0.023657\n",
      "180 loss:0.020829\n"
     ]
    }
   ],
   "source": [
    "epoch = 200;\n",
    "eta = 0.5;\n",
    "for i in range(epoch):   \n",
    "    if(i%20==0):\n",
    "        print('%d loss:%f' %(i,loss(W,x,y)))\n",
    "    grad = eval_numerical_grad(W,x,y)\n",
    "    W = W - grad * eta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 loss:0.018600\n",
      "20 loss:0.016799\n",
      "40 loss:0.015315\n",
      "60 loss:0.014070\n",
      "80 loss:0.013011\n",
      "100 loss:0.012100\n",
      "120 loss:0.011307\n",
      "140 loss:0.010612\n",
      "160 loss:0.009997\n",
      "180 loss:0.009449\n"
     ]
    }
   ],
   "source": [
    "epoch = 200;\n",
    "eta = 0.5;\n",
    "for i in range(epoch):   \n",
    "    if(i%20==0):\n",
    "        print('%d loss:%f' %(i,loss(W,x,y)))\n",
    "    grad = eval_analytic_grad(W,x,y)\n",
    "    W = W - grad * eta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
