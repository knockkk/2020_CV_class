{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1. 推导softmax clf 的 analytic gradient，并采用 numpy 实现其计算\n",
    "\n",
    "##### 2. 用numerical gradient 来检验实现的正确性"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([0.2,0.3,0.1,0.5]).reshape(1,4) #x n,3072\n",
    "y = np.array([1]) #n ,1\n",
    "W = np.random.randn(3,4) #10，3072 10 n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_analytic_grad(W, xi, yi):\n",
    "    scores = xi.dot(W.T)\n",
    "    exp_scores = np.exp(scores).reshape(W.shape[0],)\n",
    "    pro_scores = exp_scores / np.sum(exp_scores)\n",
    "    grad = np.zeros(W.shape)\n",
    "    for i in range(W.shape[0]):\n",
    "        if(i==yi):\n",
    "            grad[i] = (pro_scores[i] - 1) * xi\n",
    "        else:\n",
    "            grad[i] = pro_scores[i] * xi\n",
    "    return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(W, x, y):\n",
    "    scores = x.dot(W.T)\n",
    "    exp_scores = np.exp(scores).reshape(W.shape[0],)\n",
    "    pro_scores = exp_scores / np.sum(exp_scores)\n",
    "    return -np.log(pro_scores[y])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_numerical_grad(W, xi, yi):\n",
    "    h = 1e-5\n",
    "    grad = np.zeros(W.shape)\n",
    "    for i in range(W.shape[0]):\n",
    "        for j in range(W.shape[1]):\n",
    "            fx = loss(W, xi, yi)\n",
    "            W[i][j] =  W[i][j] + h\n",
    "            fx_h = loss(W, xi, yi)\n",
    "            grad[i][j] = (fx_h - fx) / h\n",
    "    return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numerical grad: [[ 0.03771119  0.05656689  0.01885566  0.09427854]\n",
      " [-0.15482872 -0.23224296 -0.07741428 -0.38707116]\n",
      " [ 0.11711702  0.17567572  0.05855862  0.29279347]]\n",
      "analytic_grad: [[ 0.03771116  0.05656673  0.01885558  0.09427789]\n",
      " [-0.15482867 -0.232243   -0.07741433 -0.38707166]\n",
      " [ 0.11711751  0.17567627  0.05855876  0.29279378]]\n",
      "diff 3.0538085514855706e-06\n"
     ]
    }
   ],
   "source": [
    "numerical_grad = eval_numerical_grad(W,x,y)\n",
    "analytic_grad = eval_analytic_grad(W,x,y)\n",
    "print('numerical grad:',numerical_grad)\n",
    "print('analytic_grad:',analytic_grad)\n",
    "print('diff',np.sum(np.abs(numerical_grad-analytic_grad)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analytic gradient:\n",
      "[0.03771116 0.05656673 0.01885558 0.09427789]\n",
      "[-0.15482867 -0.232243   -0.07741433 -0.38707166]\n",
      "[0.11711751 0.17567627 0.05855876 0.29279378]\n",
      "Numerical gradient:\n",
      "[0.03771119 0.05656689 0.01885566 0.09427854]\n",
      "[-0.15482872 -0.23224296 -0.07741428 -0.38707116]\n",
      "[0.11711702 0.17567572 0.05855862 0.29279347]\n"
     ]
    }
   ],
   "source": [
    "print('Analytic gradient:')\n",
    "for i in range(analytic_grad.shape[0]):\n",
    "    print(analytic_grad[i])\n",
    "print('Numerical gradient:')\n",
    "for i in range(numerical_grad.shape[0]):\n",
    "    print(numerical_grad[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 loss:1.487855\n",
      "20 loss:0.224291\n",
      "40 loss:0.103308\n",
      "60 loss:0.065840\n",
      "80 loss:0.048057\n",
      "100 loss:0.037754\n",
      "120 loss:0.031055\n",
      "140 loss:0.026358\n",
      "160 loss:0.022886\n",
      "180 loss:0.020218\n"
     ]
    }
   ],
   "source": [
    "epoch = 200;\n",
    "eta = 0.5;\n",
    "for i in range(epoch):   \n",
    "    if(i%20==0):\n",
    "        print('%d loss:%f' %(i,loss(W,x,y)))\n",
    "    grad = eval_numerical_grad(W,x,y)\n",
    "    W = W - grad * eta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 loss:2.248857\n",
      "20 loss:0.292913\n",
      "40 loss:0.118931\n",
      "60 loss:0.072401\n",
      "80 loss:0.051644\n",
      "100 loss:0.040016\n",
      "120 loss:0.032614\n",
      "140 loss:0.027500\n",
      "160 loss:0.023760\n",
      "180 loss:0.020909\n"
     ]
    }
   ],
   "source": [
    "W = np.random.randn(3,4) #10，3072 10 n\n",
    "epoch = 200;\n",
    "eta = 0.5;\n",
    "for i in range(epoch):   \n",
    "    if(i%20==0):\n",
    "        print('%d loss:%f' %(i,loss(W,x,y)))\n",
    "    grad = eval_analytic_grad(W,x,y)\n",
    "    W = W - grad * eta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
